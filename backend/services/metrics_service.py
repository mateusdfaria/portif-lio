import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

@dataclass
class MetricsResult:
    """Resultado de c√°lculo de m√©tricas"""
    mape: float
    rmse: float
    smape: float
    mae: float
    mse: float
    r2: float
    bias: float
    mase: float
    details: Dict

class MetricsService:
    """Servi√ßo para c√°lculo de m√©tricas de avalia√ß√£o de previs√µes"""
    
    def __init__(self):
        self.metrics = ['mape', 'rmse', 'smape', 'mae', 'mse', 'r2', 'bias', 'mase']
    
    def calculate_mape(self, actual: np.ndarray, predicted: np.ndarray) -> float:
        """
        Calcula MAPE (Mean Absolute Percentage Error)
        MAPE = (1/n) * Œ£|actual - predicted| / |actual| * 100
        """
        actual = np.array(actual)
        predicted = np.array(predicted)
        
        # Evitar divis√£o por zero
        mask = actual != 0
        if not np.any(mask):
            return 0.0
        
        actual_masked = actual[mask]
        predicted_masked = predicted[mask]
        
        mape = np.mean(np.abs((actual_masked - predicted_masked) / actual_masked)) * 100
        return float(mape)
    
    def calculate_smape(self, actual: np.ndarray, predicted: np.ndarray) -> float:
        """
        Calcula sMAPE (Symmetric Mean Absolute Percentage Error)
        sMAPE = (1/n) * Œ£|actual - predicted| / ((|actual| + |predicted|) / 2) * 100
        """
        actual = np.array(actual)
        predicted = np.array(predicted)
        
        # Evitar divis√£o por zero
        denominator = (np.abs(actual) + np.abs(predicted)) / 2
        denominator = np.where(denominator == 0, 1e-8, denominator)
        
        smape = np.mean(np.abs(actual - predicted) / denominator) * 100
        return float(smape)
    
    def calculate_rmse(self, actual: np.ndarray, predicted: np.ndarray) -> float:
        """
        Calcula RMSE (Root Mean Square Error)
        RMSE = sqrt(mean((actual - predicted)¬≤))
        """
        actual = np.array(actual)
        predicted = np.array(predicted)
        
        mse = np.mean((actual - predicted) ** 2)
        rmse = np.sqrt(mse)
        return float(rmse)
    
    def calculate_mae(self, actual: np.ndarray, predicted: np.ndarray) -> float:
        """
        Calcula MAE (Mean Absolute Error)
        MAE = mean(|actual - predicted|)
        """
        actual = np.array(actual)
        predicted = np.array(predicted)
        
        mae = np.mean(np.abs(actual - predicted))
        return float(mae)
    
    def calculate_mse(self, actual: np.ndarray, predicted: np.ndarray) -> float:
        """
        Calcula MSE (Mean Square Error)
        MSE = mean((actual - predicted)¬≤)
        """
        actual = np.array(actual)
        predicted = np.array(predicted)
        
        mse = np.mean((actual - predicted) ** 2)
        return float(mse)
    
    def calculate_r2(self, actual: np.ndarray, predicted: np.ndarray) -> float:
        """
        Calcula R¬≤ (Coefficient of Determination)
        R¬≤ = 1 - (SS_res / SS_tot)
        """
        actual = np.array(actual)
        predicted = np.array(predicted)
        
        ss_res = np.sum((actual - predicted) ** 2)
        ss_tot = np.sum((actual - np.mean(actual)) ** 2)
        
        if ss_tot == 0:
            return 0.0
        
        r2 = 1 - (ss_res / ss_tot)
        return float(r2)
    
    def calculate_bias(self, actual: np.ndarray, predicted: np.ndarray) -> float:
        """
        Calcula Bias (Mean Error)
        Bias = mean(predicted - actual)
        """
        actual = np.array(actual)
        predicted = np.array(predicted)
        
        bias = np.mean(predicted - actual)
        return float(bias)
    
    def calculate_mase(self, actual: np.ndarray, predicted: np.ndarray, 
                      seasonal_period: int = 7) -> float:
        """
        Calcula MASE (Mean Absolute Scaled Error)
        MASE = MAE / MAE_naive_seasonal
        """
        actual = np.array(actual)
        predicted = np.array(predicted)
        
        if len(actual) <= seasonal_period:
            return float('inf')
        
        # MAE do modelo
        mae_model = self.calculate_mae(actual, predicted)
        
        # MAE do modelo naive sazonal
        naive_errors = []
        for i in range(seasonal_period, len(actual)):
            naive_pred = actual[i - seasonal_period]
            naive_errors.append(abs(actual[i] - naive_pred))
        
        if len(naive_errors) == 0:
            return float('inf')
        
        mae_naive = np.mean(naive_errors)
        
        if mae_naive == 0:
            return float('inf')
        
        mase = mae_model / mae_naive
        return float(mase)
    
    def calculate_all_metrics(self, actual: List[float], predicted: List[float], 
                            seasonal_period: int = 7) -> MetricsResult:
        """
        Calcula todas as m√©tricas de avalia√ß√£o
        """
        actual = np.array(actual)
        predicted = np.array(predicted)
        
        # Validar entradas
        if len(actual) != len(predicted):
            raise ValueError("Arrays de actual e predicted devem ter o mesmo tamanho")
        
        if len(actual) == 0:
            raise ValueError("Arrays n√£o podem estar vazios")
        
        # Calcular m√©tricas
        mape = self.calculate_mape(actual, predicted)
        smape = self.calculate_smape(actual, predicted)
        rmse = self.calculate_rmse(actual, predicted)
        mae = self.calculate_mae(actual, predicted)
        mse = self.calculate_mse(actual, predicted)
        r2 = self.calculate_r2(actual, predicted)
        bias = self.calculate_bias(actual, predicted)
        mase = self.calculate_mase(actual, predicted, seasonal_period)
        
        # Detalhes adicionais
        details = {
            'n_samples': len(actual),
            'actual_mean': float(np.mean(actual)),
            'actual_std': float(np.std(actual)),
            'predicted_mean': float(np.mean(predicted)),
            'predicted_std': float(np.std(predicted)),
            'actual_min': float(np.min(actual)),
            'actual_max': float(np.max(actual)),
            'predicted_min': float(np.min(predicted)),
            'predicted_max': float(np.max(predicted)),
            'correlation': float(np.corrcoef(actual, predicted)[0, 1]) if len(actual) > 1 else 0.0,
            'seasonal_period': seasonal_period
        }
        
        return MetricsResult(
            mape=mape,
            rmse=rmse,
            smape=smape,
            mae=mae,
            mse=mse,
            r2=r2,
            bias=bias,
            mase=mase,
            details=details
        )
    
    def evaluate_forecast_quality(self, metrics: MetricsResult) -> Dict[str, str]:
        """
        Avalia a qualidade da previs√£o baseada nas m√©tricas
        """
        quality = {}
        
        # Avaliar MAPE
        if metrics.mape < 10:
            quality['mape'] = 'Excelente'
        elif metrics.mape < 20:
            quality['mape'] = 'Boa'
        elif metrics.mape < 50:
            quality['mape'] = 'Aceit√°vel'
        else:
            quality['mape'] = 'Ruim'
        
        # Avaliar sMAPE
        if metrics.smape < 10:
            quality['smape'] = 'Excelente'
        elif metrics.smape < 20:
            quality['smape'] = 'Boa'
        elif metrics.smape < 50:
            quality['smape'] = 'Aceit√°vel'
        else:
            quality['smape'] = 'Ruim'
        
        # Avaliar R¬≤
        if metrics.r2 > 0.9:
            quality['r2'] = 'Excelente'
        elif metrics.r2 > 0.7:
            quality['r2'] = 'Boa'
        elif metrics.r2 > 0.5:
            quality['r2'] = 'Aceit√°vel'
        else:
            quality['r2'] = 'Ruim'
        
        # Avaliar MASE
        if metrics.mase < 0.5:
            quality['mase'] = 'Excelente'
        elif metrics.mase < 1.0:
            quality['mase'] = 'Boa'
        elif metrics.mase < 1.5:
            quality['mase'] = 'Aceit√°vel'
        else:
            quality['mase'] = 'Ruim'
        
        return quality
    
    def compare_models(self, results: List[Tuple[str, MetricsResult]]) -> Dict:
        """
        Compara m√∫ltiplos modelos baseado nas m√©tricas
        """
        if not results:
            return {}
        
        comparison = {}
        
        # Encontrar melhor modelo para cada m√©trica
        for metric_name in ['mape', 'smape', 'rmse', 'mae', 'r2']:
            if metric_name == 'r2':
                # Para R¬≤, maior √© melhor
                best_model = max(results, key=lambda x: getattr(x[1], metric_name))
            else:
                # Para outras m√©tricas, menor √© melhor
                best_model = min(results, key=lambda x: getattr(x[1], metric_name))
            
            comparison[f'best_{metric_name}'] = {
                'model': best_model[0],
                'value': getattr(best_model[1], metric_name)
            }
        
        # Calcular ranking geral (baseado em sMAPE)
        sorted_results = sorted(results, key=lambda x: x[1].smape)
        comparison['ranking'] = [
            {
                'model': model_name,
                'smape': metrics.smape,
                'mape': metrics.mape,
                'rmse': metrics.rmse,
                'r2': metrics.r2
            }
            for model_name, metrics in sorted_results
        ]
        
        return comparison
    
    def generate_metrics_report(self, metrics: MetricsResult, model_name: str = "Modelo") -> str:
        """
        Gera relat√≥rio textual das m√©tricas
        """
        quality = self.evaluate_forecast_quality(metrics)
        
        report = f"""
üìä RELAT√ìRIO DE M√âTRICAS - {model_name.upper()}
{'='*50}

üìà M√âTRICAS PRINCIPAIS:
‚Ä¢ MAPE (Mean Absolute Percentage Error): {metrics.mape:.2f}% - {quality['mape']}
‚Ä¢ sMAPE (Symmetric MAPE): {metrics.smape:.2f}% - {quality['smape']}
‚Ä¢ RMSE (Root Mean Square Error): {metrics.rmse:.2f} - {quality['rmse'] if 'rmse' in quality else 'N/A'}
‚Ä¢ MAE (Mean Absolute Error): {metrics.mae:.2f}
‚Ä¢ R¬≤ (Coefficient of Determination): {metrics.r2:.3f} - {quality['r2']}

üìä M√âTRICAS ADICIONAIS:
‚Ä¢ MSE (Mean Square Error): {metrics.mse:.2f}
‚Ä¢ Bias (Mean Error): {metrics.bias:.2f}
‚Ä¢ MASE (Mean Absolute Scaled Error): {metrics.mase:.2f} - {quality['mase']}

üìã DETALHES ESTAT√çSTICOS:
‚Ä¢ Amostras: {metrics.details['n_samples']}
‚Ä¢ M√©dia dos valores reais: {metrics.details['actual_mean']:.2f}
‚Ä¢ Desvio padr√£o dos valores reais: {metrics.details['actual_std']:.2f}
‚Ä¢ M√©dia das previs√µes: {metrics.details['predicted_mean']:.2f}
‚Ä¢ Desvio padr√£o das previs√µes: {metrics.details['predicted_std']:.2f}
‚Ä¢ Correla√ß√£o: {metrics.details['correlation']:.3f}

üéØ INTERPRETA√á√ÉO:
‚Ä¢ MAPE < 10%: Excelente | 10-20%: Bom | 20-50%: Aceit√°vel | >50%: Ruim
‚Ä¢ sMAPE < 10%: Excelente | 10-20%: Bom | 20-50%: Aceit√°vel | >50%: Ruim
‚Ä¢ R¬≤ > 0.9: Excelente | 0.7-0.9: Bom | 0.5-0.7: Aceit√°vel | <0.5: Ruim
‚Ä¢ MASE < 0.5: Excelente | 0.5-1.0: Bom | 1.0-1.5: Aceit√°vel | >1.5: Ruim
"""
        
        return report

# Inst√¢ncia global do servi√ßo
metrics_service = MetricsService()
